{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "506a6363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28c5a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "z= torch.FloatTensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fbcb82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "hypothesis= F.softmax(z,dim=0)    # exp(i) / sigma exp(i)    class= i\n",
    "print(hypothesis) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be6a9006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2160, 0.2008, 0.2087, 0.2175, 0.1570],\n",
      "        [0.1970, 0.2387, 0.1424, 0.2827, 0.1392],\n",
      "        [0.1766, 0.2761, 0.1202, 0.1399, 0.2872]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z= torch.rand(3,5,requires_grad=True)\n",
    "hypothesis= F.softmax(z,dim=1)    \n",
    "print(hypothesis) # this value is predict value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f146b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "y= torch.randint(5,(3,)).long()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bbe4fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot= torch.zeros_like(hypothesis)\n",
    "y_one_hot.scatter_(1,y.unsqueeze(1),1)  # 1 of first factor=dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6101d431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7499, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost= (y_one_hot * (-torch.log(hypothesis))).sum(dim=1).mean() # cross entropy loss\n",
    "print(cost)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4fe2d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5324, -1.6054, -1.5669, -1.5257, -1.8514],\n",
       "        [-1.6248, -1.4324, -1.9490, -1.2633, -1.9720],\n",
       "        [-1.7338, -1.2871, -2.1190, -1.9665, -1.2475]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(z,dim=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b9c0f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7499, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.nll_loss(F.log_softmax(z,dim=1),y) # NLL= negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ad6fc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7499, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(z,y) # simply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ca5c25",
   "metadata": {},
   "source": [
    "## training with Low-level cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67ac39d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train= [[1,2,1,1],[2,1,3,2],[3,1,3,4],[4,1,5,5],[1,7,5,5],[1,2,5,6],\n",
    "         [1,6,6,6],[1,7,7,7]]\n",
    "y_train= [2,2,2,1,1,1,0,0]\n",
    "x_train= torch.FloatTensor(x_train)\n",
    "y_train=torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29b75790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    0 / 1000 cost: 1.098612\n",
      "epoch  100 / 1000 cost: 0.901535\n",
      "epoch  200 / 1000 cost: 0.839114\n",
      "epoch  300 / 1000 cost: 0.807826\n",
      "epoch  400 / 1000 cost: 0.788472\n",
      "epoch  500 / 1000 cost: 0.774822\n",
      "epoch  600 / 1000 cost: 0.764449\n",
      "epoch  700 / 1000 cost: 0.756191\n",
      "epoch  800 / 1000 cost: 0.749398\n",
      "epoch  900 / 1000 cost: 0.743671\n",
      "epoch 1000 / 1000 cost: 0.738749\n"
     ]
    }
   ],
   "source": [
    "w= torch.zeros((4,3),requires_grad=True)\n",
    "b= torch.zeros(1, requires_grad=True)\n",
    "optimizer= optim.SGD([w,b],lr=0.1)\n",
    "nb_epochs=1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    hypothesis= F.softmax(x_train.matmul(w)+b,dim=1)\n",
    "    y_one_hot=torch.zeros_like(hypothesis)\n",
    "    y_one_hot.scatter_(1,y_train.unsqueeze(1),1)\n",
    "    cost= (y_one_hot * -torch.log(F.softmax(hypothesis, dim=1))).sum(dim=1).mean()\n",
    "    # z=x_train.matmul(w) + b\n",
    "    # cost= F.cross_entropy(z,y_train) also possible\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 ==0:\n",
    "        print('epoch {:4d} / {} cost: {:.6f}'.format(epoch,nb_epochs,cost.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531c38b",
   "metadata": {},
   "source": [
    "## using class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58d840e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class scm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear= nn.Linear(4,3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "model= scm() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a63a726f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    0 / 1000 cost: 1.181581\n",
      "epoch  100 / 1000 cost: 0.641062\n",
      "epoch  200 / 1000 cost: 0.556476\n",
      "epoch  300 / 1000 cost: 0.502024\n",
      "epoch  400 / 1000 cost: 0.457680\n",
      "epoch  500 / 1000 cost: 0.418170\n",
      "epoch  600 / 1000 cost: 0.381115\n",
      "epoch  700 / 1000 cost: 0.344991\n",
      "epoch  800 / 1000 cost: 0.308668\n",
      "epoch  900 / 1000 cost: 0.271994\n",
      "epoch 1000 / 1000 cost: 0.243457\n"
     ]
    }
   ],
   "source": [
    "optimizer= optim.SGD(model.parameters(),lr=0.1)\n",
    "nb_epochs=1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    prediction= model(x_train)\n",
    "    cost= F.cross_entropy(prediction,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 ==0:\n",
    "        print('epoch {:4d} / {} cost: {:.6f}'.format(epoch,nb_epochs,cost.item()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
